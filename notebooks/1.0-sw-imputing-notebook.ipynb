{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Last update: August 2023*  \n",
    "*Contact:* fdrs@ifrc.org or simon.weiss@ifrc.org  \n",
    "*FDRS focal point:* Simon Weiss, FDRS Data Analyst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to deal with Missing Values? FDRS imputation methodology in python\n",
    "-----------\n",
    "\n",
    "\n",
    "The FDRS is ambitious and wide reaching. Although the data quality and reporting are improving each year, data is missing for some National Societies. As a result, some data fluctuations may be misleading: trend lines can drop for a given year when there is missing data, and some National Societies are excluded from the total and then appear again in another year.In order to better represent the network and better count everyone, FDRS implement every year data imputation techniques. \n",
    "\n",
    "The purpose of this notebook is to apply the method selected by the FDRS team and to interact with the FDRS backoffice with a post method to publish the imputed values. \n",
    "The ingested data is  replicated and displayed on the website https://data.ifrc.org/FDRS/ and used in FDRS research such as [Everyone count report](https://data-api.ifrc.org/documents/noiso/Everyone%20Counts%20Report%202022%20EN.pdf). \n",
    "\n",
    "Therefore this is not a research notebook but a production notebook. It aims to simply expose the FDRS methodology of imputation step by step. A related python script has been created: [imputing.py]() which allows to launch this methodology once. \n",
    "\n",
    "The approach chosen was to replace the 2019, 2020 and 2021 missing data as well as to apply two different techniques according to the indicator categories, in the previous years all NSs reported their data then no input technique was employed.   \n",
    "\n",
    "The imputing applies only to main indicators and does not apply to disaggregated levels to maintain consistency across years. A detailed description of the methodology is available in the pdf [Missing Data]()\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\n",
    "1. [Import Necessary Packages](#import-packages)\n",
    "2. [Selecting Production and Staging Environment](#environment-selection)\n",
    "3. [Data Retrieval](#data-retrieval)\n",
    "4. [Data Imputation](#data-imputation)\n",
    "5. [POST Method to Update Back-end](#post-method)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Initial Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the necessary Python modules and libraries for our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pandas import json_normalize\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, let's select the desired environment. This will determine the configuration settings we'll use throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_environment(config_prod_path, config_staging_path):\n",
    "    \"\"\"Select the environment and load the respective configuration.\"\"\"\n",
    "    env = ''\n",
    "    while env not in ['staging', 'prod']:\n",
    "        env = input(\"Select environment ('staging' or 'prod'): \").lower()\n",
    "        if env == 'prod':\n",
    "            confirm = input(\"WARNING: You've selected the PRODUCTION environment. Are you sure? (yes/no) \")\n",
    "            if confirm.lower() != 'yes':\n",
    "                print(\"Switching to 'staging' by default for safety.\")\n",
    "                env = 'staging'\n",
    "    try:\n",
    "        with open(config_prod_path if env == 'prod' else config_staging_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        raise Exception(f\"Configuration file for {env} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_prod_path = '../src/config/config_prod.json'\n",
    "config_staging_path = '../src/config/config_staging.json'\n",
    "config = select_environment(config_prod_path, config_staging_path)\n",
    "print(f\"Loaded configuration for {config['ENV']} environment.\")\n",
    "user_email = input('Please provide your FDRS data analyst email: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import necessary modules for data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(os.path.join(module_path, \"src\", \"data\"))\n",
    "from fdrsapi import api_function, baseline, api_function_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate fdrs codebook and store it in references folder\n",
    "\n",
    "base_url = config['BASE_URL']\n",
    "KPI = json_normalize(data=requests.get(f\"{base_url}indicator?apiKey={config['API_KEY_PUBLIC']}\").json())\n",
    "codebook_path = os.path.join('..', 'references', 'codebook.xlsx')\n",
    "KPI.to_excel(codebook_path)\n",
    "codebook = pd.read_excel(codebook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook = pd.read_excel(r\"..\\references\\codebook.xlsx\")\n",
    "kpi_reach = codebook.query('KPI_Note == \"NS Reach - CPD\"')[\"KPI_Code\"].to_list() + [\"KPI_TrainFA_Tot\", \"KPI_DonBlood_Tot\"]\n",
    "metadata_kpis = codebook.query('KPI_Note == \"Metadata\"')[\"KPI_Code\"].tolist()\n",
    "kpi_gov = [\"KPI_GB_Tot\", \"KPI_PeopleVol_Tot\", \"KPI_PStaff_Tot\", \"KPI_noLocalUnits\"]\n",
    "kpi_fi = [\"KPI_IncomeLC_CHF\", \"KPI_expenditureLC_CHF\"]\n",
    "kpi_gov_fi_code = kpi_gov + kpi_fi\n",
    "\n",
    "exclude_kpis = [\"KPI_Year\", \"DON_Code\", \"KPI_Id\"]\n",
    "metadata_kpis = [kpi for kpi in metadata_kpis if kpi not in exclude_kpis]\n",
    "kpi_code = kpi_reach + kpi_gov_fi_code + metadata_kpis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting nb of considered KPIs\n",
    "print(\"number of total KPI considered:\", len(kpi_code)-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Get data from FDRS API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\"]\n",
    "time_series = api_function(years, kpi_code, config)\n",
    "original_data = time_series.copy()\n",
    "original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Analyzing FDRS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store the counts\n",
    "metadata_counts = pd.DataFrame()\n",
    "\n",
    "# Loop over each metadata KPI\n",
    "for kpi in metadata_kpis:\n",
    "    if \"Date\" in kpi:  # Check if the KPI is a date indicator\n",
    "        yes_count = time_series[time_series[kpi].notna()].groupby('KPI_Year').size()\n",
    "    else:\n",
    "        yes_count = time_series[time_series[kpi] == 1.0].groupby('KPI_Year').size()\n",
    "\n",
    "    no_count = time_series[time_series[kpi] == 0.0].groupby('KPI_Year').size()\n",
    "    na_count = time_series[time_series[kpi].isna()].groupby('KPI_Year').size()\n",
    "    \n",
    "    # Construct a temporary DataFrame and append it to the main DataFrame\n",
    "    temp_df = pd.DataFrame({\n",
    "        'KPI': kpi,\n",
    "        'True': yes_count,\n",
    "        'False': no_count,\n",
    "        'NA': na_count\n",
    "    }).reset_index()\n",
    "    \n",
    "    metadata_counts = metadata_counts._append(temp_df, ignore_index=True)\n",
    "\n",
    "# Pivot the dataframe for a better view\n",
    "pivot_metadata_counts = metadata_counts.pivot(index='KPI', columns='KPI_Year')\n",
    "\n",
    "# Display the metadata counts table\n",
    "# Specify the order of KPIs to display at the top\n",
    "priority_kpis = ['KPI_WasSubmitted', 'validated']\n",
    "\n",
    "# Reorder the index to have priority KPIs at the top\n",
    "pivot_metadata_counts = pivot_metadata_counts.reindex(priority_kpis + [kpi for kpi in pivot_metadata_counts.index if kpi not in priority_kpis])\n",
    "\n",
    "# Display the pivoted table for comparison\n",
    "pivot_metadata_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series['KPI_FirstSubmitDate'] = pd.to_datetime(time_series['KPI_FirstSubmitDate'], errors='coerce')\n",
    "\n",
    "# Extract month\n",
    "time_series['Month'] = time_series['KPI_FirstSubmitDate'].dt.month\n",
    "\n",
    "# Group by year and month, then compute the cumulative count\n",
    "grouped_cumulative = time_series.groupby(['KPI_Year', 'Month']).size().groupby(level=0).cumsum().reset_index(name='Cumulative Count')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "for year in time_series['KPI_Year'].unique():\n",
    "    subset = grouped_cumulative[grouped_cumulative['KPI_Year'] == year]\n",
    "    plt.plot(subset['Month'], subset['Cumulative Count'], label=year, marker='o')\n",
    "\n",
    "plt.title('Cumulative Number of KPI_FirstSubmitDate Submissions Over the Year')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Cumulative Number of Submissions')\n",
    "plt.xticks(np.arange(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series['KPI_NSR_SubmitDate'] = pd.to_datetime(time_series['KPI_NSR_SubmitDate'], errors='coerce')\n",
    "\n",
    "\n",
    "# Extract month\n",
    "time_series['Month'] = time_series['KPI_NSR_SubmitDate'].dt.month\n",
    "\n",
    "# Group by year and month, then compute the cumulative count\n",
    "grouped_cumulative = time_series.groupby(['KPI_Year', 'Month']).size().groupby(level=0).cumsum().reset_index(name='Cumulative Count')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "for year in time_series['KPI_Year'].unique():\n",
    "    subset = grouped_cumulative[grouped_cumulative['KPI_Year'] == year]\n",
    "    plt.plot(subset['Month'], subset['Cumulative Count'], label=year, marker='o')\n",
    "\n",
    "plt.title('Cumulative Number of NS Reach Sections Submissions Over the Year')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Cumulative Number of Submissions')\n",
    "plt.xticks(np.arange(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(df, kpi_reach_code, kpi_gov_fi_code, selected_year):\n",
    "    \"\"\"\n",
    "    Impute missing data in the dataframe based on various conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The original dataframe with potential missing values.\n",
    "        kpi_reach_code (list): List of KPI codes related to reach.\n",
    "        kpi_gov_fi_code (list): List of KPI codes related to governance and financial indicators.\n",
    "        selected_year (int): The year for which to perform the imputation.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with imputed values.\n",
    "    \"\"\"\n",
    "\n",
    "    data = df.copy()\n",
    "    condition_selected_year = data['KPI_Year'] == float(selected_year)\n",
    "    condition_not_submitted = data['KPI_WasSubmitted'] != 1.0\n",
    "    condition_nsgs_not_submitted = data['KPI_NSGS_WasSubmitted'] != 1.0\n",
    "\n",
    "    for col in kpi_reach_code:\n",
    "        mask = condition_selected_year & condition_not_submitted & data[col].isna()\n",
    "        if mask.any():\n",
    "            data[col] = data.groupby(\"NSO_DON_name\")[col].transform(lambda x: x.fillna(x.mean(), limit=3))\n",
    "\n",
    "    for col in kpi_gov_fi_code:\n",
    "        mask = condition_selected_year & condition_not_submitted & data[col].isna()\n",
    "        if mask.any():\n",
    "            data[col] = data.groupby(\"NSO_DON_name\")[col].transform(lambda x: x.fillna(method='ffill', limit=3))\n",
    "\n",
    "    condition_special = (data['KPI_WasSubmitted'] == 1.0) & condition_nsgs_not_submitted\n",
    "    for col in kpi_gov:\n",
    "        mask = condition_selected_year & condition_special & data[col].isna()\n",
    "        if mask.any():\n",
    "            data[col] = data.groupby(\"NSO_DON_name\")[col].transform(lambda x: x.fillna(method='ffill', limit=3))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform imputation for the year 2022\n",
    "imputed_data_2022 = impute_data(time_series, kpi_reach, kpi_gov_fi_code, 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data_2021 = impute_data(time_series, kpi_reach, kpi_gov_fi_code, 2021)\n",
    "imputed_data_2022 = impute_data(time_series, kpi_reach, kpi_gov_fi_code, 2022)\n",
    "\n",
    "# Compare imputed and original (non-imputed) data\n",
    "print(\"\\nComparison (Original vs. Imputed) for 2022:\")\n",
    "print(\"Original:\", time_series[time_series[\"KPI_Year\"] == 2022][[\"NSO_DON_name\", \"KPI_PeopleVol_Tot\"]].sum())\n",
    "print(\"Imputed:\", imputed_data_2022[imputed_data_2022[\"KPI_Year\"] == 2022][[\"NSO_DON_name\", \"KPI_PeopleVol_Tot\"]].sum())\n",
    "\n",
    "print(\"\\nComparison (Original vs. Imputed) for 2021:\")\n",
    "print(\"Original:\", time_series[time_series[\"KPI_Year\"] == 2021][[\"NSO_DON_name\", \"KPI_PeopleVol_Tot\"]].sum())\n",
    "print(\"Imputed:\", imputed_data_2021[imputed_data_2021[\"KPI_Year\"] == 2021][[\"NSO_DON_name\", \"KPI_PeopleVol_Tot\"]].sum())\n",
    "\n",
    "# Compare imputed and original (non-imputed) data\n",
    "print(\"\\nComparison (Original vs. Imputed) for 2022:\")\n",
    "print(\"Original:\", time_series[time_series[\"KPI_Year\"] == 2022][[\"NSO_DON_name\", \"KPI_IncomeLC_CHF\"]].sum())\n",
    "print(\"Imputed:\", imputed_data_2022[imputed_data_2022[\"KPI_Year\"] == 2022][[\"NSO_DON_name\", \"KPI_IncomeLC_CHF\"]].sum())\n",
    "\n",
    "print(\"\\nComparison (Original vs. Imputed) for 2021:\")\n",
    "print(\"Original:\", time_series[time_series[\"KPI_Year\"] == 2021][[\"NSO_DON_name\", \"KPI_IncomeLC_CHF\"]].sum())\n",
    "print(\"Imputed:\", imputed_data_2021[imputed_data_2021[\"KPI_Year\"] == 2021][[\"NSO_DON_name\", \"KPI_IncomeLC_CHF\"]].sum())\n",
    "\n",
    "\n",
    "# Compare imputed and original (non-imputed) data for KPI_ReachDRER_CPD_IP\n",
    "print(\"\\nComparison (Original vs. Imputed) for KPI_ReachDRER_CPD in 2022:\")\n",
    "print(\"Original:\", time_series[time_series[\"KPI_Year\"] == 2022][[\"NSO_DON_name\", \"KPI_ReachDRER_CPD\"]].sum())\n",
    "print(\"Imputed:\", imputed_data_2022[imputed_data_2022[\"KPI_Year\"] == 2022][[\"NSO_DON_name\", \"KPI_ReachDRER_CPD\"]].sum())\n",
    "\n",
    "\n",
    "print(\"\\nComparison (Original vs. Imputed) for KPI_ReachDRER_CPD in 2021:\")\n",
    "print(\"Original:\", time_series[time_series[\"KPI_Year\"] == 2021][[\"NSO_DON_name\", \"KPI_ReachDRER_CPD\"]].sum())\n",
    "print(\"Imputed:\", imputed_data_2021[imputed_data_2021[\"KPI_Year\"] == 2021][[\"NSO_DON_name\", \"KPI_ReachDRER_CPD\"]].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter to NSO DON Name DIN001 and 2022 for PeopleVol\n",
    "\n",
    "imputed_data_2022[(imputed_data_2022[\"KPI_Year\"] == 2022) & (imputed_data_2022[\"NSO_DON_name\"] == \"Indian Red Cross Society\")][[\"NSO_DON_name\", \"KPI_PeopleVol_Tot\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. POST Method to Update Back-end\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_post(original_dataframe, imputed_dataframe, kpi_list, selected_year):\n",
    "    # Filter dataframes by the selected year\n",
    "    original_subset = original_dataframe[original_dataframe[\"KPI_Year\"] == selected_year].copy()\n",
    "    imputed_subset = imputed_dataframe[imputed_dataframe[\"KPI_Year\"] == selected_year].copy()\n",
    "\n",
    "    # For each KPI, iterate through each row in the imputed dataframe\n",
    "    for kpi in kpi_list:\n",
    "        for index, row in imputed_subset.iterrows():\n",
    "            don_code = row['KPI_DON_code']\n",
    "            original_value = original_subset[(original_subset['KPI_DON_code'] == don_code)][kpi].values[0]\n",
    "            imputed_value = row[kpi]\n",
    "\n",
    "            # Determine the source value based on the comparison\n",
    "            source = 'NSI' if original_value == imputed_value else 'I'\n",
    "            imputed_subset.at[index, f\"{kpi}_source\"] = source\n",
    "\n",
    "    return imputed_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data_prepared = prepare_data_for_post(original_data, imputed_data_2022, kpi_reach + kpi_gov_fi_code, 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_imputed_data(dataframe, kpi_list, selected_year):\n",
    "    total_kpis = len(kpi_list)\n",
    "    total_dons = len(set(dataframe[\"KPI_DON_code\"]))\n",
    "    total_requests = total_kpis * total_dons\n",
    "\n",
    "    sleep_time = 0.5\n",
    "    estimated_time = total_requests * sleep_time / 60\n",
    "    print(f\"Estimated time for completion: {estimated_time:.2f} minutes.\")\n",
    "    \n",
    "    failed_posts = []  # To keep track of any failed POST requests\n",
    "\n",
    "    with tqdm(total=total_requests, desc=\"Posting Data\", unit=\"Request\") as pbar:\n",
    "        for kpi in kpi_list:\n",
    "            kpi_ip = kpi + \"_IP\"\n",
    "            source_column_name = kpi + \"_source\"\n",
    "            for kpi_don_code in set(dataframe[\"KPI_DON_code\"]):\n",
    "                value_row = dataframe[(dataframe[\"KPI_Year\"] == selected_year) & (dataframe[\"KPI_DON_code\"] == kpi_don_code)]\n",
    "                \n",
    "                value = value_row[kpi].values[0]\n",
    "                source_value = value_row[source_column_name].values[0]\n",
    "\n",
    "                if pd.isna(value):\n",
    "                    value = \"\"\n",
    "                elif isinstance(value, float):\n",
    "                    value = int(value)\n",
    "\n",
    "                url = f\"{config['BASE_URL']}ImputedKPI?apiKey={config['API_KEY_PRIVATE']}&kpicode={kpi_ip}&year={selected_year}&don_code={kpi_don_code}&value={value}&source={source_value}&user={user_email}\"\n",
    "                \n",
    "                response = requests.post(url)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "                # Check response status code\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Failed to post data for KPI: {kpi}, DON: {kpi_don_code}. Server responded with {response.status_code}: {response.text}\")\n",
    "                    failed_posts.append((kpi, kpi_don_code, response.status_code, response.text))\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "    # Once done, if there were any failures, print a summary\n",
    "    if failed_posts:\n",
    "        print(\"\\nSummary of failed POSTs:\")\n",
    "        for kpi, don, status_code, message in failed_posts:\n",
    "            print(f\"KPI: {kpi}, DON: {don} failed with {status_code}: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_imputed_data(imputed_data_prepared, kpi_reach + kpi_gov_fi_code, 2022)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "f5cd46ac39ef9f755e58296a5a0dafef5534f685873da8eb47c2c22f7cec4531"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
